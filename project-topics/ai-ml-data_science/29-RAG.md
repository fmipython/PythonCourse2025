# Теми за проекти: RAG системи

## Описание

RAG (Retrieval-Augmented Generation) системите са комбинация от големи езикови модели (LLMs) и методи за извличане на информация (retrieval). Те позволяват на моделите да генерират текст, базиран на външни източници на данни, което подобрява качеството и релевантността на отговорите.

Такива системи работят в две последователни стъпки:
1. **Извличане на информация (Retrieval)**. "Външното знание" на бота. Целта на тази стъпка е да се извлече най-релевантния контекст, който след това да бъде подаден на езиковия модел. Това може да бъде търсене в интернет, ровене из налични локално файлове, търсене в бази данни или бази знания.
2. **Генериране на текст (Generation)**. Отговорът на бота. Извлечените най-релевантни парчета с информация се прикрепят към първоначалната заявка на потребителя и така се подават като цял един user prompt към езиковия модел (LLM).

## Теми

1.  Обобщение на новини и статии от интернет по дадена тема
2.  Асистент за пазарни анализи и финансови новини
3.  Персонализиран гид за пътувания (събира информация за дестинации, атракции, ресторанти)
4.  Асистент за сравнение на продукти и цени от различни онлайн магазини
5.  Чатбот за техническа поддръжка (Support Chatbot)
6.  Персонализиран асистент за учене


## Задължителни изисквания
- Само за теми 1-4: събиране на информация от интернет по дадена тема чрез публични API-та. Това се случва в стъпката за извличане на информация (retrieval).
- Само за теми 5 и 6: предварително запазване на корпус от документи/файлове, от които да се извлича информацията, с което и отделен скрипт, който да я подготвя (парсва, почиства, разбива на парчета, индексира/векторизира и съхранява в база данни, вж. техническите бележки долу). В стъпката за извличане на информация (retrieval) се търси в тази база данни.
- Използване на голям езиков модел (LLM) за генериране на отговори, като се подава извлечената информация като контекст.
- Указание към езиковия модел да посочва източниците на информация (т.е. линкове към статиите или имената на файловете) при генерирането на отговори, под формата на markdown връзка с валиден линк. Примери:
    - `Според статията от (Вестник 24 Часа)[https://24chasa.bg/statiq], ...`
    - `Python е динамично-типизиран, интерпретируем език за програмиране. (Лекция 1)[file:///path/to/your/local/files/lecture1.md]` (абсолютният път към файла може да бъде "хардкоднат" в скрипта за обработка на корпуса с файлове)
- Продължаване на диалога (conversation history) между потребителя и системата, като се запазват последните няколко съобщения в разговора (в база данни) и се подават към езиковия модел като контекст при всяка нова заявка.
- API, което позволява на потребителя да взаимодейства с RAG системата чрез подаване на заявки и получаване на отговори:
    - задължителен endpoint за подаване на заявка и получаване на отговор от системата чрез streaming (Server-Sent Events) (за повече инфо виж техническите бележки долу) на всеки токен в JSON формата `{"success": true|false, "content": "..."}`, където:
      - при успешно генериране на отговор, `success` е `true`, а `content` съдържа следващия токен от отговора;
      - при прихваната грешка, `success` е `false`, а `content` съдържа съобщението на грешката;
- Кратко демо/тестово приложение, което освен че позволява на потребителя да изпробва RAG системата, също така дава опция да се промени системния prompt (инструкцията към езиковия модел) и други параметри като температура, максимален брой токени и др.


## Пожелателни (бонус) функционалности
- Поддръжка на потребителска автентикация (регистрация и логин)


## Технически бележки и препоръки

1. **Retrieval**:
   - За задачите, в които се изисква извличане на информация, намираща се в набор от файлове, препоръчваме използването на векторни бази данни (vector databases) като ChromaDB, Pinecone и др. Те предоставят наготово алгоритмите за разбиване на корпуса на парчета от текст (chunking), модели за векторизация на тези парчета (embedding models) и бързо търсене на най-подходящите такива (similarity search), като всичко това може да се случи с [няколко реда код](https://docs.trychroma.com/docs/overview/getting-started). Приложим е и друг по-традиционен метод за постигане на целта с класическо индексиране посредством методи като TF-IDF & BM25 (чрез Elasticsearch например), както и хибриден подход. Имплементирайте който прецените вие с подходяща библиотека.
   - Обикновените текстови файлове са по-лесни за парсване и дават по-добри резултати от PDF-и.
   - За задачите, в които се изисква извличане на информация от интернет, може да използвате публични API-та като това на DuckDuckGo. Имат и [библиотека](https://pypi.org/project/duckduckgo-search/#4-news)]. Няма нужда да минавате през уеб скрейпинг, освен ако не искате да съберете специфична информация от определени сайтове. Чрез ограчаване на събраната информация до подходящ брой статии/токени, може да я подадете към LLM-а без да има нужда да се разделя и векторизира, както в горния случай.
2. **Generation**:
   - Има и такива LLM-и с отворен достъп, но поради големите ресурси, които се изискват за локалното им пускане, препоръчваме директно извикване на модели чрез съответните им API-та (като тези на OpenAI, Anthropic, Google Vertex, DeepSeek, Qwen и др.). (Повечето са платени, но не вярваме повече от един-два долара да изразходите за целите на всички задачи. Все пак внимавайте с броя данни и сметнете очаквания разход по токени преди да изпробвате.)
   - Streaming отговорите се реализират чрез Server-Sent Events (SSE). Повечето LLM API-та поддържат такъв режим на работа. Във FastAPI например това се реализира чрез [`StreamingResponse`](https://fastapi.tiangolo.com/advanced/custom-response/?h=streamingresponse#streamingresponse), на който се подава генератор.
3. Prompt engineering:
    - LLM-ите имат два различни prompt-a, които се указват: системен (system) и потребителски (user). Системният се подава веднъж в началото на всеки разговор и инструктира модела как да се държи (напр. `Ти си асистент за пазарни анализи и финансови новини...`), а потребителският е самата текуща заявка на потребителя, към която и трябва да се добавя извлечената информация като контекст (напр. `#Context: ... \n\n #User query: Чудя се дали да инвестирам в акции на Nvidia. Искам да ми дадеш пълен анализ относно приходите и разходите на компанията, както и сентимент от последните новини.`).
    - Експериментирайте с различни формулировки на системния prompt, за да видите как това влияе на отговорите на модела. Може да се позовавате на официалните ръководства за [GPT-5](https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide) или за [Claude](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/overview?ref=prompt-engineer.com).
    - "Reasoning" моделите (например GPT-5 и GPT-5.1) са много по-бавни и често хабят повече токени (следователно и пари). Препоръчваме да започнете с нормален модел (например GPT-4.1) и ако след изчерпателна експериментация не сте доволни от качеството на отговорите, чак тогава да преминете към някой reasoning модел. Понеже, особено при OpenAI, именуването им е безразборно, проверявайте първо в [официалната документация](https://platform.openai.com/docs/models) кой модел е reasoning и кой не, както и дали може да се спира това свойство))
4. Относно демо приложението: `Streamlit` предлага прост чат интерфейс графичен компонент, с поддръжка на streaming: [тук има инструкции и пример](https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps#build-a-simple-chatbot-gui-with-streaming).

## **ЗАБРАНЕНИ БИБЛИОТЕКИ**
`langchain`, `langgraph`, `llama_index`, `agno`, `adk`, `pydantic-ai` и всякакви агентизирани библиотеки за работа на по-високо ниво с LLM агенти и RAG системи. С тях просто задачата може да се напише твърде бързо и лесно, без много собствена имплементация и мислене, а и няма какво повече да добавим като изисквания, така че да има как качествено да оценим Python знанията ви. Ако използвате такива библиотеки, максималните точки за критерия "функционалност" ще бъдат наполовина.

## Препоръчителни технологии
- LLMs: `openai` или `anthropic` и др. - специфичната библиотека зависи от доставчика на модела
- База данни (за запзване на съобщенията в диалога): SQLite (чрез `sqlite3` или `SQLAlchemy`), PostgreSQL (чрез `psycopg2` или `SQLAlchemy`), MongoDB (чрез `pymongo`) или др.
- Secrets management: `pydantic-settings` или `python-dotenv`
- API: `FastAPI` или `Flask`
- Демо апликация: `streamlit`
- Работа с векторни бази данни (ако е приложимо): `chromadb` или `pinecone`
- Работа с класически индекси (ако е приложимо): `elasticsearch`
- Търсене на уебстраници и съдъжрание (ако е приложимо): `duckduckgo-search`
- Scrape-ване на уеб страници (ако е необходимо): `requests`/`aiohttp`, `BeautifulSoup`, `scrapy`
